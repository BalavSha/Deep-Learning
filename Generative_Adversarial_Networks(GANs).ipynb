{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoF24ArPX++Q/iKUpuYL8m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BalavSha/Deep-Learning/blob/main/Generative_Adversarial_Networks(GANs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center>**Generative Adversarial Network(GAN)**</center>"
      ],
      "metadata": {
        "id": "5tTN3jw2uzfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Generation**"
      ],
      "metadata": {
        "id": "ut97QLwtwgWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a sequential model for a **LSTM (Long Short-Term Memory)** neural network:"
      ],
      "metadata": {
        "id": "oEJPUrLgz1sQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZT7Us63tt5t"
      },
      "outputs": [],
      "source": [
        "# Create a sequential model\n",
        "regressor = Sequential()\n",
        "\n",
        "# Add the first LSTM layer with 50 units and 'relu' activation function\n",
        "# Set 'return_sequences' to True as we will add more LSTM layers\n",
        "# The input shape is the shape of the training data\n",
        "regressor.add(LSTM(units=50, activation='relu', return_sequences=True, \\\n",
        "                   input_shape=(X_train.shape[1], 5)))\n",
        "\n",
        "# Add a dropout layer to prevent overfitting\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Add the second LSTM layer with 60 units and 'relu' activation function\n",
        "# Set 'return_sequences' to True as we will add more LSTM layers\n",
        "regressor.add(LSTM(units=60, activation='relu', return_sequences=True))\n",
        "\n",
        "# Add another dropout layer\n",
        "regressor.add(Dropout(0.3))\n",
        "\n",
        "# Add the third LSTM layer with 80 units and 'relu' activation function\n",
        "# Set 'return_sequences' to True as we will add more LSTM layers\n",
        "regressor.add(LSTM(units=80, activation='relu', return_sequences=True))\n",
        "\n",
        "# Add another dropout layer\n",
        "regressor.add(Dropout(0.4))\n",
        "\n",
        "# Add the fourth LSTM layer with 120 units and 'relu' activation function\n",
        "regressor.add(LSTM(units=120, activation='relu'))\n",
        "\n",
        "# Add another dropout layer\n",
        "regressor.add(Dropout(0.5))\n",
        "\n",
        "# Add a dense layer with 1 unit as the output layer\n",
        "regressor.add(Dense(units=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extending NLP Sequence Models to Generate Text:**\n",
        "\n",
        "**Steps Involved are:**\n",
        "\n",
        "*   **Dataset cleaning** encompasses the conversion of the case to lowercase, removing punctuation.\n",
        "\n",
        "*   **Tokenization** is breaking up a character sequence into specified units called tokens.\n",
        "\n",
        "*   **Padding** is a way to make input sentences of different sizes the same by padding them.\n",
        "\n",
        "*   **Padding the sequences** refers to making sure that the sequences have a uniform length.\n",
        "\n",
        "*   **Stemming** is truncating words down to their stem. For example, the words *rainy* and *raining* both have the stem *rain*."
      ],
      "metadata": {
        "id": "tH2ABx0Q01yR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***--> Dataset Cleaning:***"
      ],
      "metadata": {
        "id": "E48dGits2JVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Create a function to clean text and returns list of words after cleaning. Converting all words in lowercase and encoding for character standardization.*"
      ],
      "metadata": {
        "id": "j0Fvm3tj2x03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to clean text\n",
        "def clean_text(txt):\n",
        "\n",
        "    # Remove punctuation from the text and convert it to lowercase\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "\n",
        "    # Encode the text as utf-8 and decode it as ascii, ignoring any errors\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\", 'ignore')\n",
        "    \n",
        "    # Return the cleaned text\n",
        "    return txt\n",
        "\n",
        "# Apply the clean_text function to all headlines in the 'all_headlines' list\n",
        "corpus = [clean_text(x) for x in all_headlines]\n",
        "\n",
        "# Print the first 10 elements of the 'corpus' list\n",
        "corpus[:10]"
      ],
      "metadata": {
        "id": "-rqLOmpB07dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***--> Generating a Sequence and Tokenization:***"
      ],
      "metadata": {
        "id": "wk0SlKbj3cYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tokenizer class from keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Create a Tokenizer object\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Define a function to get sequences of tokens from a corpus\n",
        "def get_seq_of_tokens(corpus):\n",
        "\n",
        "    # Fit the tokenizer on the corpus\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    # Get the total number of words in the tokenizer's word index\n",
        "    all_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Initialize an empty list to store the input sequences\n",
        "    input_seq = []\n",
        "\n",
        "    # Iterate over each line in the corpus\n",
        "    for line in corpus:\n",
        "\n",
        "        # Convert the line to a sequence of tokens\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "        # Iterate over each token in the token list\n",
        "        for i in range(1, len(token_list)):\n",
        "\n",
        "            # Get the n-gram sequence of tokens\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            \n",
        "            # Append the n-gram sequence to the input sequences list\n",
        "            input_seq.append(n_gram_sequence)\n",
        "\n",
        "    # Return the input sequences and the total number of words\n",
        "    return input_seq, all_words\n",
        "\n",
        "# Get the sequences of tokens and the total number of words from the 'corpus' list\n",
        "your_sequences, all_words = get_seq_of_tokens(corpus)\n",
        "\n",
        "# Print the first 10 elements of the 'your_sequences' list\n",
        "your_sequences[:10]"
      ],
      "metadata": {
        "id": "DWeSv_9K4BbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***--> Padding Sequences***"
      ],
      "metadata": {
        "id": "6KbzFgxC5gSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Create a function to pad each Sequence to make their lengths equal to match length of longest sequence in the text Corpus*"
      ],
      "metadata": {
        "id": "6WP9d_HY56z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate padded sequences\n",
        "def generate_padded_sequences(input_seq):\n",
        "\n",
        "    # Get the maximum sequence length\n",
        "    max_sequence_len = max([len(x) for x in input_seq])\n",
        "\n",
        "    # Pad the input sequences with zeros, in the beginning of sentences\n",
        "    input_seq = np.array(pad_sequences(input_seq, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "    # Split the input sequences into predictors and label\n",
        "    predictors, label = input_seq[:, :-1], input_seq[:, -1]\n",
        "\n",
        "    # Convert the label to categorical format\n",
        "    label = keras.utils.to_categorical(label, num_classes=all_words)\n",
        "\n",
        "    # Return the predictors, label, and maximum sequence length\n",
        "    return predictors, label, max_sequence_len\n",
        "\n",
        "# Generate padded sequences from the 'your_sequences' list\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(your_sequences)"
      ],
      "metadata": {
        "id": "mdqj_6N_5lUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "\n",
        "------"
      ],
      "metadata": {
        "id": "glMTDOow629u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Building LSTM Model to generate new news headlines using the historical news information from the dataset**"
      ],
      "metadata": {
        "id": "JEiUxUXa7UEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Important required libraries*"
      ],
      "metadata": {
        "id": "y1ehPCj17-GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pad_sequences function from the Keras library\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Import the Sequential class from the Keras library\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Import the Embedding, LSTM, Dense, and Dropout layers from the Keras library\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Import the keras.utils module as ku\n",
        "import tensorflow.keras.utils as ku\n",
        "\n",
        "# Import the Tokenizer class from the Keras library\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Import the pandas library as pd\n",
        "import pandas as pd\n",
        "# Import the numpy library as np\n",
        "import numpy as np\n",
        "\n",
        "# Import the EarlyStopping callback from the Keras library\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Import the string and os modules\n",
        "import string, os\n",
        "\n",
        "# Import the warnings module\n",
        "import warnings\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "XwCAjoer65Er"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Load the Article dataset and display its length:*\n",
        "--> **Load the Articles.csv dataset in the colab folder first**"
      ],
      "metadata": {
        "id": "Bwr9iNdv9ey9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the directory path\n",
        "your_dir = '/content/'\n",
        "\n",
        "# Initialize an empty list to store the headlines\n",
        "all_headlines = []\n",
        "\n",
        "# Iterate over each file in the directory\n",
        "for filename in os.listdir(your_dir):\n",
        "\n",
        "    # Check if the filename contains 'Articles'\n",
        "    if 'Articles' in filename:\n",
        "\n",
        "        # Read the CSV file into a DataFrame\n",
        "        article_df = pd.read_csv(your_dir + filename)\n",
        "\n",
        "        # Extend the headlines list with the values from the 'headline' column\n",
        "        all_headlines.extend(list(article_df.headline.values))\n",
        "\n",
        "        # Break out of the loop\n",
        "        break\n",
        "\n",
        "# Filter out any headlines with the value 'Unknown'\n",
        "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "\n",
        "# Print the length of the 'all_headlines' list\n",
        "len(all_headlines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3wMtGw18mf9",
        "outputId": "030ce84b-1c0d-4516-b4c4-80987a691e67"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "831"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Create a function to return a list of cleaned words. Convert the text to lowercase and encode it with with \"utf-8\" for character standardization.*"
      ],
      "metadata": {
        "id": "wg3lJHgy-OlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to clean text\n",
        "def clean_text(txt):\n",
        "\n",
        "    # Remove punctuation from the text and convert it to lowercase\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "\n",
        "    # Encode the text as utf-8 and decode it as ascii, ignoring any errors\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\", 'ignore')\n",
        "\n",
        "    # Return the cleaned text\n",
        "    return txt\n",
        "\n",
        "# Apply the clean_text function to all headlines in the 'all_headlines' list\n",
        "corpus = [clean_text(x) for x in all_headlines]\n",
        "\n",
        "# Print headlines of index(60 - 80) of the 'corpus' list\n",
        "corpus[60:80]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3TuewXu9tIW",
        "outputId": "492276a0-3f29-4a89-f8a8-4cedf19e098d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lets go for a win on opioids',\n",
              " 'floridas vengeful governor',\n",
              " 'how to end the politicization of the courts',\n",
              " 'when dr king came out against vietnam',\n",
              " 'britains trains dont run on time blame capitalism',\n",
              " 'questions for no license plates here using art to transcend prison walls',\n",
              " 'dry spell',\n",
              " 'are there subjects that should be offlimits to artists or to certain artists in particular',\n",
              " 'that is great television',\n",
              " 'thinking in code',\n",
              " 'how gorsuchs influence could be greater than his vote',\n",
              " 'new york today how to ease a hangover',\n",
              " 'trumps gifts to china',\n",
              " 'at penn station rail mishap spurs large and lasting headache',\n",
              " 'chemical attack on syrians ignites worlds outrage',\n",
              " 'adventure is still on babbos menu',\n",
              " 'swimming in the fast lane',\n",
              " 'a national civics exam',\n",
              " 'obama adviser is back in the political cross hairs',\n",
              " 'the hippies have won']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Create a function to convert a given corpus of text into a list of input sequences of tokens. It does this by converting each line in the corpus into a sequence of tokens.*"
      ],
      "metadata": {
        "id": "JH_1UZiQ_7JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Tokenizer object\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Define a function to get sequences of tokens from a corpus\n",
        "def get_seq_of_tokens(corpus):\n",
        "\n",
        "    # Fit the tokenizer on the corpus\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    # Get the total number of words in the tokenizer's word index\n",
        "    all_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Initialize an empty list to store the input sequences\n",
        "    input_seq = []\n",
        "\n",
        "    # Iterate over each line in the corpus\n",
        "    for line in corpus:\n",
        "\n",
        "        # Convert each line to a sequence of tokens\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "        # Iterate over each token in the token list\n",
        "        for i in range(1, len(token_list)):\n",
        "\n",
        "            # Get the n-gram sequence of tokens\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "\n",
        "            # Append the n-gram sequence to the input sequences list\n",
        "            input_seq.append(n_gram_sequence)\n",
        "\n",
        "    # Return the input sequences and the total number of words\n",
        "    return input_seq, all_words\n",
        "\n",
        "# Get the sequences of tokens and the total number of words from the 'corpus' list\n",
        "your_sequences, all_words = get_seq_of_tokens(corpus)\n",
        "\n",
        "# Print the first 20 elements of the 'your_sequences' list\n",
        "your_sequences[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrVw-7vF_fCy",
        "outputId": "6f5941d1-c74e-4d60-b04c-229608dd34fc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[169, 17],\n",
              " [169, 17, 665],\n",
              " [169, 17, 665, 367],\n",
              " [169, 17, 665, 367, 4],\n",
              " [169, 17, 665, 367, 4, 2],\n",
              " [169, 17, 665, 367, 4, 2, 666],\n",
              " [169, 17, 665, 367, 4, 2, 666, 170],\n",
              " [169, 17, 665, 367, 4, 2, 666, 170, 5],\n",
              " [169, 17, 665, 367, 4, 2, 666, 170, 5, 667],\n",
              " [6, 80],\n",
              " [6, 80, 1],\n",
              " [6, 80, 1, 668],\n",
              " [6, 80, 1, 668, 10],\n",
              " [6, 80, 1, 668, 10, 669],\n",
              " [670, 671],\n",
              " [670, 671, 129],\n",
              " [670, 671, 129, 672],\n",
              " [673, 674],\n",
              " [673, 674, 368],\n",
              " [673, 674, 368, 675]]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Create a function to pad each Sequence to make their lengths equal to match length of longest sequence in the text Corpus:*"
      ],
      "metadata": {
        "id": "gv5NjLgFCqis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate padded sequences\n",
        "def generate_padded_sequences(input_seq):\n",
        "\n",
        "    # Get the maximum sequence length\n",
        "    max_sequence_len = max([len(x) for x in input_seq])\n",
        "\n",
        "    # Pad the input sequences with zeros in the beginning\n",
        "    input_seq = np.array(pad_sequences(input_seq, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "    # Split the input sequences into predictors and label\n",
        "    predictors, label = input_seq[:, :-1], input_seq[:, -1]\n",
        "\n",
        "    # Convert the label to categorical format\n",
        "    label = ku.to_categorical(label, num_classes=all_words)\n",
        "\n",
        "    # Return the predictors, label, and maximum sequence length\n",
        "    return predictors, label, max_sequence_len\n",
        "\n",
        "# Generate padded sequences from the 'inp_seq' list\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(your_sequences)"
      ],
      "metadata": {
        "id": "U3ru6aSvBzBL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Prepare the Model architecture for compiling and training:*"
      ],
      "metadata": {
        "id": "uGxQdTLHD3Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to create the model\n",
        "def create_model(max_sequence_len, all_words):\n",
        "\n",
        "  # set the input length\n",
        "  input_len = max_sequence_len - 1\n",
        "\n",
        "  # create a Sequential model\n",
        "  model = Sequential()\n",
        "\n",
        "  # add embedding layer to create word vectors for incoming words\n",
        "  model.add(Embedding(all_words, 10, input_length=input_len))\n",
        "\n",
        "  # add an LSTM layer with 100 units\n",
        "  model.add(LSTM(100))\n",
        "\n",
        "  # add a dropout layer to prevent overfitting\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  # add a output Dense layer with \"all_words\" units and \"softmax\" activation function\n",
        "  model.add(Dense(all_words, activation=\"softmax\"))\n",
        "\n",
        "  # compile the model with \"categorical_crossentropy\" and \"Adam\" optimizer\n",
        "  model.compile(\n",
        "      optimizer = \"adam\",\n",
        "      loss = \"categorical_crossentropy\"\n",
        "              )\n",
        "  \n",
        "  # return the model\n",
        "  return model\n",
        "\n",
        "# create the model using the \"max_sequence_len\" and \"all_words\" variables\n",
        "model = create_model(max_sequence_len, all_words)"
      ],
      "metadata": {
        "id": "QDL_dSNfDlKi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the summary of model architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB-rY4W5Ep4H",
        "outputId": "06fa3743-a26d-44bd-cfb1-65c8a3c1a9ee"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 18, 10)            24220     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               44400     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2422)              244622    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 313,242\n",
            "Trainable params: 313,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Fit the model with data*"
      ],
      "metadata": {
        "id": "kfrovE6pGS7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    predictors,\n",
        "    label,\n",
        "    epochs = 200,\n",
        "    verbose = 5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDZcFyjSGOCh",
        "outputId": "2a03d3d6-f84b-47e8-e4cd-e044b52122dc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "Epoch 2/200\n",
            "Epoch 3/200\n",
            "Epoch 4/200\n",
            "Epoch 5/200\n",
            "Epoch 6/200\n",
            "Epoch 7/200\n",
            "Epoch 8/200\n",
            "Epoch 9/200\n",
            "Epoch 10/200\n",
            "Epoch 11/200\n",
            "Epoch 12/200\n",
            "Epoch 13/200\n",
            "Epoch 14/200\n",
            "Epoch 15/200\n",
            "Epoch 16/200\n",
            "Epoch 17/200\n",
            "Epoch 18/200\n",
            "Epoch 19/200\n",
            "Epoch 20/200\n",
            "Epoch 21/200\n",
            "Epoch 22/200\n",
            "Epoch 23/200\n",
            "Epoch 24/200\n",
            "Epoch 25/200\n",
            "Epoch 26/200\n",
            "Epoch 27/200\n",
            "Epoch 28/200\n",
            "Epoch 29/200\n",
            "Epoch 30/200\n",
            "Epoch 31/200\n",
            "Epoch 32/200\n",
            "Epoch 33/200\n",
            "Epoch 34/200\n",
            "Epoch 35/200\n",
            "Epoch 36/200\n",
            "Epoch 37/200\n",
            "Epoch 38/200\n",
            "Epoch 39/200\n",
            "Epoch 40/200\n",
            "Epoch 41/200\n",
            "Epoch 42/200\n",
            "Epoch 43/200\n",
            "Epoch 44/200\n",
            "Epoch 45/200\n",
            "Epoch 46/200\n",
            "Epoch 47/200\n",
            "Epoch 48/200\n",
            "Epoch 49/200\n",
            "Epoch 50/200\n",
            "Epoch 51/200\n",
            "Epoch 52/200\n",
            "Epoch 53/200\n",
            "Epoch 54/200\n",
            "Epoch 55/200\n",
            "Epoch 56/200\n",
            "Epoch 57/200\n",
            "Epoch 58/200\n",
            "Epoch 59/200\n",
            "Epoch 60/200\n",
            "Epoch 61/200\n",
            "Epoch 62/200\n",
            "Epoch 63/200\n",
            "Epoch 64/200\n",
            "Epoch 65/200\n",
            "Epoch 66/200\n",
            "Epoch 67/200\n",
            "Epoch 68/200\n",
            "Epoch 69/200\n",
            "Epoch 70/200\n",
            "Epoch 71/200\n",
            "Epoch 72/200\n",
            "Epoch 73/200\n",
            "Epoch 74/200\n",
            "Epoch 75/200\n",
            "Epoch 76/200\n",
            "Epoch 77/200\n",
            "Epoch 78/200\n",
            "Epoch 79/200\n",
            "Epoch 80/200\n",
            "Epoch 81/200\n",
            "Epoch 82/200\n",
            "Epoch 83/200\n",
            "Epoch 84/200\n",
            "Epoch 85/200\n",
            "Epoch 86/200\n",
            "Epoch 87/200\n",
            "Epoch 88/200\n",
            "Epoch 89/200\n",
            "Epoch 90/200\n",
            "Epoch 91/200\n",
            "Epoch 92/200\n",
            "Epoch 93/200\n",
            "Epoch 94/200\n",
            "Epoch 95/200\n",
            "Epoch 96/200\n",
            "Epoch 97/200\n",
            "Epoch 98/200\n",
            "Epoch 99/200\n",
            "Epoch 100/200\n",
            "Epoch 101/200\n",
            "Epoch 102/200\n",
            "Epoch 103/200\n",
            "Epoch 104/200\n",
            "Epoch 105/200\n",
            "Epoch 106/200\n",
            "Epoch 107/200\n",
            "Epoch 108/200\n",
            "Epoch 109/200\n",
            "Epoch 110/200\n",
            "Epoch 111/200\n",
            "Epoch 112/200\n",
            "Epoch 113/200\n",
            "Epoch 114/200\n",
            "Epoch 115/200\n",
            "Epoch 116/200\n",
            "Epoch 117/200\n",
            "Epoch 118/200\n",
            "Epoch 119/200\n",
            "Epoch 120/200\n",
            "Epoch 121/200\n",
            "Epoch 122/200\n",
            "Epoch 123/200\n",
            "Epoch 124/200\n",
            "Epoch 125/200\n",
            "Epoch 126/200\n",
            "Epoch 127/200\n",
            "Epoch 128/200\n",
            "Epoch 129/200\n",
            "Epoch 130/200\n",
            "Epoch 131/200\n",
            "Epoch 132/200\n",
            "Epoch 133/200\n",
            "Epoch 134/200\n",
            "Epoch 135/200\n",
            "Epoch 136/200\n",
            "Epoch 137/200\n",
            "Epoch 138/200\n",
            "Epoch 139/200\n",
            "Epoch 140/200\n",
            "Epoch 141/200\n",
            "Epoch 142/200\n",
            "Epoch 143/200\n",
            "Epoch 144/200\n",
            "Epoch 145/200\n",
            "Epoch 146/200\n",
            "Epoch 147/200\n",
            "Epoch 148/200\n",
            "Epoch 149/200\n",
            "Epoch 150/200\n",
            "Epoch 151/200\n",
            "Epoch 152/200\n",
            "Epoch 153/200\n",
            "Epoch 154/200\n",
            "Epoch 155/200\n",
            "Epoch 156/200\n",
            "Epoch 157/200\n",
            "Epoch 158/200\n",
            "Epoch 159/200\n",
            "Epoch 160/200\n",
            "Epoch 161/200\n",
            "Epoch 162/200\n",
            "Epoch 163/200\n",
            "Epoch 164/200\n",
            "Epoch 165/200\n",
            "Epoch 166/200\n",
            "Epoch 167/200\n",
            "Epoch 168/200\n",
            "Epoch 169/200\n",
            "Epoch 170/200\n",
            "Epoch 171/200\n",
            "Epoch 172/200\n",
            "Epoch 173/200\n",
            "Epoch 174/200\n",
            "Epoch 175/200\n",
            "Epoch 176/200\n",
            "Epoch 177/200\n",
            "Epoch 178/200\n",
            "Epoch 179/200\n",
            "Epoch 180/200\n",
            "Epoch 181/200\n",
            "Epoch 182/200\n",
            "Epoch 183/200\n",
            "Epoch 184/200\n",
            "Epoch 185/200\n",
            "Epoch 186/200\n",
            "Epoch 187/200\n",
            "Epoch 188/200\n",
            "Epoch 189/200\n",
            "Epoch 190/200\n",
            "Epoch 191/200\n",
            "Epoch 192/200\n",
            "Epoch 193/200\n",
            "Epoch 194/200\n",
            "Epoch 195/200\n",
            "Epoch 196/200\n",
            "Epoch 197/200\n",
            "Epoch 198/200\n",
            "Epoch 199/200\n",
            "Epoch 200/200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3fe13fbe20>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a function that will generate a headline given a starting seed text, the number of words to generate, the model, and the maximum sequence length.** <br>\n",
        "\n",
        "*The function will include a for loop to iterate over the number of words to generate.* <br>\n",
        "\n",
        "*In each iteration, the tokenizer will tokenize the text, and then pad the sequence before predicting the next word in the sequence.* <br>\n",
        "\n",
        "*Next, the iteration will convert the token back into a word and add it to the sentence.* <br>\n",
        "\n",
        "*Once the for loop completes, the generated headline will be returned:*"
      ],
      "metadata": {
        "id": "tIWxmOCkIJNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate text\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "\n",
        "    # Iterate for the number of next words to generate\n",
        "    for _ in range(next_words):\n",
        "\n",
        "        # Convert the seed text to a sequence of tokens\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "        # Pad the token list with zeros\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "        # Make a prediction using the model\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "\n",
        "        # Initialize an empty string to store the output word\n",
        "        output_word = \"\"\n",
        "\n",
        "        # Iterate over each word and index in the tokenizer's word index\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "\n",
        "            # Check if the index matches the predicted value\n",
        "            if index == np.argmax(predicted):\n",
        "\n",
        "                # Set the output word to the current word\n",
        "                output_word = word\n",
        "\n",
        "                # Break out of the loop\n",
        "                break\n",
        "\n",
        "        # Append the output word to the seed text\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    # Return the generated text with title casing\n",
        "    return seed_text.title()"
      ],
      "metadata": {
        "id": "82cjq4vFGlkG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Use the function above to output text generation as below:*"
      ],
      "metadata": {
        "id": "E0eRODUMJ0Oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using the 'generate_text' function and print the result\n",
        "print(generate_text(\"Balav is a Machine Learning\", 5, model, max_sequence_len))\n",
        "print()\n",
        "print(generate_text(\"europe looks to\", 8, model, max_sequence_len))\n",
        "print()\n",
        "print(generate_text(\"best way\", 10, model, max_sequence_len))\n",
        "print()\n",
        "print(generate_text(\"homeless in\", 10, model, max_sequence_len))\n",
        "print()\n",
        "print(generate_text(\"unexpected results\", 10, model, max_sequence_len))\n",
        "print()\n",
        "print(generate_text(\"Are there subjects that should be offlimits to artists or to certain artists in\", 1, model, max_sequence_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzuyErZgJzKX",
        "outputId": "8d7b2017-c1b1-4e05-8c20-8fb5fd1035b7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balav Is A Machine Learning Of The Border On Official\n",
            "\n",
            "Europe Looks To Feel Really American Try Some Velveeta To Use\n",
            "\n",
            "Best Way To Move Forward Sets May You 2017 Warm 2017 Men\n",
            "\n",
            "Homeless In The Center Of A Classic Nasty Woman On An Elegy\n",
            "\n",
            "Unexpected Results Isnt That Could Expand Access To Fossil Fuels Thinks Nuisance\n",
            "\n",
            "Are There Subjects That Should Be Offlimits To Artists Or To Certain Artists In Particular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "\n",
        "------"
      ],
      "metadata": {
        "id": "Gl4d9V41O-Oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center>**Generative Adversarial Networks(GANs)**</center>\n",
        "\n",
        "**--> GANs are networks that generates new, synthetic data by learning patterns and underlying representations from a training dataset.**\n",
        "\n",
        "--> For instance, generating a new Images based on a given Images dataset."
      ],
      "metadata": {
        "id": "gvzvT5EePjUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XdluL2iwO12Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}